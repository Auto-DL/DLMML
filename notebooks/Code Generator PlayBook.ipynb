{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.json_to_dict import MakeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Nested JSON\n",
    "with open('SampleJSONs' + os.sep + 'f2b_new_cats_vs_dogs_example.json') as f:\n",
    "    inputs = json.load(f)\n",
    "inputs = MakeDict(inputs).parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for all constants\n",
    "\n",
    "TEST_DIR = \"..\" + os.sep + 'test' + os.sep\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"..\" + os.sep + 'data' + os.sep)\n",
    "CATS_DIR = os.path.join(DATA_DIR, 'dogs_and_cats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_table = {\n",
    "    'Conv2D' : 'model.add(Conv2D())',\n",
    "    'MaxPooling2D' : 'model.add(MaxPooling2D())',\n",
    "    'Dense': 'model.add(Dense())',\n",
    "    'Flatten': 'model.add(Flatten())',\n",
    "    'sgd': 'opt = SGD(lr=0.001, momentum=0.9)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imports():\n",
    "    \"\"\"\n",
    "    imports the needed modules \n",
    "    \"\"\"\n",
    "    try:\n",
    "        return \\\n",
    "        \"\"\"\n",
    "# File generated by DLMML Parser\n",
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\"\"\"\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_sequential():\n",
    "    \"\"\"\n",
    "    initializes the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return \\\n",
    "'\\nmodel = Sequential()\\n'\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_input(inputs):\n",
    "    \"\"\"\n",
    "    req in input json -\n",
    "    dataset-path -> path to dataset\n",
    "    image-augment -> params for image augmentation if required\n",
    "    params -> params for building generator\n",
    "    \"\"\"\n",
    "    base = inputs['dataset']['path']\n",
    "    test_dir = os.path.join(base, 'test')\n",
    "    train_dir = os.path.join(base, 'train')\n",
    "    \n",
    "    paths = \\\n",
    "\"\"\"\n",
    "\n",
    "base = '{}'\n",
    "train_dir = os.path.join(base, 'train')\n",
    "test_dir = os.path.join(base, 'test')\n",
    "\"\"\".format(inputs['dataset']['path'])\n",
    "\n",
    "    generators = \\\n",
    "\"\"\"\n",
    "augment = {}\n",
    "kwargs = {}\n",
    "\n",
    "train_datagen = ImageDataGenerator(**augment)\n",
    "test_datagen = ImageDataGenerator(**augment)\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, **kwargs)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, **kwargs)\n",
    "\n",
    "\"\"\".format(inputs['image']['augment'], inputs['image']['params'])\n",
    "\n",
    "    return paths+generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(input_dict):\n",
    "    \"\"\"\n",
    "    Parser which adds layers (mostly)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # init_sequential()\n",
    "        layers = input_dict['layers']\n",
    "        generated_code = ''\n",
    "\n",
    "        #TODO: Take care of indentation and stuff if making functions\n",
    "        #TODO: Ordering in dict (work around -> ordered_dict)\n",
    "\n",
    "        for layer in layers:\n",
    "            name = layer.get('name', None)\n",
    "            curr_layer = symbol_table[name]\n",
    "            args = layer.copy()\n",
    "            args.pop('name')\n",
    "            args = str(args)\n",
    "            curr_layer = curr_layer[:-2] + '**' + args + curr_layer[-2:]\n",
    "            generated_code += curr_layer + '\\n'\n",
    "        print()\n",
    "        return generated_code\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(inputs):\n",
    "    \"\"\"\n",
    "    Compiles model along with adding optimizer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        opt = inputs.get(\"optimizer\", \"sgd\")\n",
    "        opt_code = symbol_table[opt]\n",
    "        # let loss and metrics be a compulsory fields for the user?\n",
    "        loss = inputs.get(\"loss\")\n",
    "        metrics = str(inputs.get(\"metrics\"))\n",
    "\n",
    "        return '\\n' + opt_code + '\\n' + \\\n",
    "\"model.compile(optimizer=opt, loss='{}', metrics={})\".format(loss, metrics)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_plots_and_summary(fname=\"test\"):\n",
    "    return \\\n",
    "\"\"\"\n",
    "def summarize_diagnostics(history, save_plots):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "\n",
    "    # save plot to file\n",
    "    if save_plots:\n",
    "        filename = '{}'\n",
    "        pyplot.savefig(filename + '_plot.png')\n",
    "        pyplot.close()\n",
    "\"\"\".format(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(inputs):\n",
    "    \"\"\"\n",
    "    Adds code required to train and evaluate model\n",
    "    Works only if:\n",
    "        - train_data's generator is train_generator\n",
    "        - test_data's generator is called test_generator\n",
    "        - Only epochs and verbose taken from json\n",
    "    \"\"\"\n",
    "    try:\n",
    "            epochs = inputs.get('epochs', 20) #decide default\n",
    "            verbose = inputs.get('verbose', 0)\n",
    "            fit_generator = \\\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\\\n\\\\n ==========Fitting Model========== \\\\n\")\n",
    "history = model.fit_generator(\n",
    "                    train_generator, \n",
    "                    steps_per_epoch=len(train_generator),\n",
    "                    validation_data=test_generator, \n",
    "                    validation_steps=len(test_generator), \n",
    "                    epochs={}, \n",
    "                    verbose={}\n",
    "                )\n",
    "\"\"\".format(epochs, verbose)\n",
    "            \n",
    "            eval_generator = \\\n",
    "\"\"\"            \n",
    "\n",
    "print(\"\\\\n\\\\n ==========Evalutating Model========== \\\\n\")\n",
    "_, acc = model.evaluate_generator(test_generator, steps=len(test_generator), verbose={})\n",
    "print('\\\\n\\\\nACCURACY:  %.3f \\\\n\\\\n' % (acc * 100.0))\n",
    "\"\"\".format(verbose)\n",
    "            return fit_generator + eval_generator\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_summarize_model(inputs):\n",
    "    if eval(inputs.get('plot', False)):\n",
    "        return \\\n",
    "\"\"\"\n",
    "\n",
    "# Plotting graphs and Summarizing Model\n",
    "summarize_diagnostics(history, {})\n",
    "\"\"\".format(eval(inputs.get('save_plots', False)))\n",
    "\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(file_name, input_dict):\n",
    "    \"\"\"\n",
    "    file_name: string (with extension), generated code \\\n",
    "               will be saved as file_name in the test dir\n",
    "    input_dict: Dict generated by parsing json to make \\\n",
    "                nested dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inputs = input_dict\n",
    "        with open(TEST_DIR + file_name, 'w') as f:\n",
    "            f.write(get_imports())\n",
    "            f.write(add_plots_and_summary())\n",
    "            f.write(image_input(inputs))\n",
    "            f.write(init_sequential())\n",
    "            f.write(parse(inputs))\n",
    "            f.write(compile_model(inputs))\n",
    "            f.write(train_evaluate_model(inputs))\n",
    "            f.write(plot_and_summarize_model(inputs))\n",
    "        print(\"Code generated in file test/\" + file_name)\n",
    "    except:\n",
    "        print(\"Exception occured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Code generated in file test/test.py\n"
     ]
    }
   ],
   "source": [
    "write_to_file('test.py', inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
