{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from makeNested import make_nestedJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'path': '../data/dogs_and_cats', 'type': 'image'},\n",
      " 'epochs': 5,\n",
      " 'image': {'augment': {'height_shift_range': 0.2,\n",
      "                       'horizontal_flip': 'True',\n",
      "                       'rescale': 0.0039215,\n",
      "                       'rotation_range': 40,\n",
      "                       'width_shift_range': 0.2},\n",
      "           'params': {'batch_size': 64,\n",
      "                      'class_mode': 'binary',\n",
      "                      'target_size': [200, 200]}},\n",
      " 'layers': [{'activation': 'relu',\n",
      "             'filters': 32,\n",
      "             'input_shape': [200, 200, 3],\n",
      "             'kernel_size': [3, 3],\n",
      "             'name': 'Conv2D',\n",
      "             'padding': 'same'},\n",
      "            {'name': 'MaxPooling2D', 'pool_size': [2, 2]},\n",
      "            {'name': 'Flatten'},\n",
      "            {'activation': 'relu',\n",
      "             'kernel_initializer': 'he_uniform',\n",
      "             'name': 'Dense',\n",
      "             'units': 128},\n",
      "            {'activation': 'sigmoid', 'name': 'Dense', 'units': 1}],\n",
      " 'loss': 'binary_crossentropy',\n",
      " 'metrics': ['accuracy'],\n",
      " 'optimizer': 'sgd',\n",
      " 'plot': 'True',\n",
      " 'save_plots': 'True',\n",
      " 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "# Make Nested JSON\n",
    "with open('f2b_new_cats_vs_dogs_example.json') as f:\n",
    "    inputs = json.load(f)\n",
    "pprint(make_nestedJSON(inputs).parse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for all constants\n",
    "\n",
    "TEST_DIR = \"..\" + os.sep + 'test' + os.sep\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"..\" + os.sep + 'data' + os.sep)\n",
    "CATS_DIR = os.path.join(DATA_DIR, 'dogs_and_cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "my_json = \\\n",
    "\"\"\"\n",
    "{\n",
    "\t\"layers\": {\n",
    "\t\t\"conv2d\": {\n",
    "\t\t\t\"filters\": 32,\n",
    "\t\t\t\"kernel_size\": [3, 3],\n",
    "\t\t\t\"activation\": \"relu\",\n",
    "\t\t\t\"padding\": \"same\",\n",
    "\t\t\t\"input_shape\": [200, 200, 3]\n",
    "\t\t},\n",
    "\n",
    "\t\t\"mp2\": {\n",
    "\t\t\t\"pool_size\": [2, 2]\n",
    "\t\t},\n",
    "\n",
    "        \"flatten\": {},\n",
    "\n",
    "        \"dense\": {\n",
    "            \"units\": 128,\n",
    "            \"activation\": \"relu\",\n",
    "            \"kernel_initializer\": \"he_uniform\"\n",
    "        },\n",
    "\n",
    "        \"dense\": {\n",
    "            \"units\": 1,\n",
    "            \"activation\": \"sigmoid\"\n",
    "        }\n",
    "\n",
    "\t},\n",
    "\n",
    "\t\"dataset\": {\n",
    "\t\t\"type\": \"image\",\n",
    "\t\t\"path\": \"../data/dogs_and_cats\"\n",
    "\t},\n",
    "\n",
    "\t\"image\": {\n",
    "\t\t\"augment\": {\n",
    "\t\t\t\"rotation_range\": 40,\n",
    "\t\t\t\"width_shift_range\": 0.2,\n",
    "\t\t\t\"height_shift_range\": 0.2,\n",
    "\t\t\t\"horizontal_flip\": \"True\",\n",
    "\t\t\t\"rescale\": 0.0039215\n",
    "\t\t}\n",
    "\t},\n",
    "\t\"params\": {\n",
    "\t\t\"target_size\": [200, 200],\n",
    "\t\t\"batch_size\": 64,\n",
    "\t\t\"class_mode\": \"binary\"\n",
    "\t},\n",
    "\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"metrics\": [\n",
    "        \"accuracy\"\n",
    "    ],\n",
    "    \"epochs\": 5,\n",
    "    \"verbose\": 1,\n",
    "    \"plot\": \"True\",\n",
    "    \"save_plots\": \"True\"\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = json.loads(my_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_table = {\n",
    "    'conv2d' : 'model.add(Conv2D())',\n",
    "    'mp2' : 'model.add(MaxPooling2D())',\n",
    "    'dense': 'model.add(Dense())',\n",
    "    'flatten': 'model.add(Flatten())',\n",
    "    'sgd': 'opt = SGD(lr=0.001, momentum=0.9)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imports():\n",
    "    \"\"\"\n",
    "    imports the needed modules \n",
    "    \"\"\"\n",
    "    try:\n",
    "        return \\\n",
    "        \"\"\"\n",
    "# File generated by DLMML Parser\n",
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\"\"\"\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_sequential():\n",
    "    \"\"\"\n",
    "    initializes the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return \\\n",
    "'\\nmodel = Sequential()\\n'\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_input(inputs):\n",
    "    \"\"\"\n",
    "    req in input json -\n",
    "    dataset-path -> path to dataset\n",
    "    image-augment -> params for image augmentation if required\n",
    "    params -> params for building generator\n",
    "    \"\"\"\n",
    "    base = inputs['dataset']['path']\n",
    "    test_dir = os.path.join(base, 'test')\n",
    "    train_dir = os.path.join(base, 'train')\n",
    "    \n",
    "    paths = \\\n",
    "'\\nbase = \\'{}\\'\\n\\\n",
    "train_dir = os.path.join(base, \\'train\\')\\n\\\n",
    "test_dir = os.path.join(base, \\'test\\')\\n'.format(inputs['dataset']['path'])\n",
    "\n",
    "    i = \\\n",
    "'\\naugment = {}\\n\\\n",
    "kwargs = {}\\n\\n\\\n",
    "train_datagen = ImageDataGenerator(**augment)\\n\\\n",
    "test_datagen = ImageDataGenerator(**augment)\\n\\\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, **kwargs)\\n\\\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, **kwargs)\\n\\n'.format(inputs['image']['augment'], inputs['params'])\n",
    "\n",
    "    return paths+i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(input_dict):\n",
    "    \"\"\"\n",
    "    Parser which adds layers (mostly)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # init_sequential()\n",
    "        input_dict = input_dict['layers']\n",
    "        generated_code = ''\n",
    "\n",
    "        #TODO: Take care of indentation and stuff if making functions\n",
    "        #TODO: Ordering in dict (work around -> ordered_dict)\n",
    "\n",
    "        for layer in input_dict.keys():\n",
    "            curr_layer = symbol_table[layer]\n",
    "            args = str(input_dict[layer])\n",
    "            curr_layer = curr_layer[:-2] + '**' + args + curr_layer[-2:]\n",
    "            generated_code += curr_layer + '\\n'\n",
    "        print()\n",
    "        return generated_code\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(inputs):\n",
    "    \"\"\"\n",
    "    Compiles model along with adding optimizer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        opt = inputs.get(\"optimizer\", \"sgd\")\n",
    "        opt_code = symbol_table[opt]\n",
    "        # let loss and metrics be a compulsory fields for the user?\n",
    "        loss = inputs.get(\"loss\")\n",
    "        metrics = str(inputs.get(\"metrics\"))\n",
    "\n",
    "        return '\\n' + opt_code + '\\n' + \\\n",
    "\"model.compile(optimizer=opt, loss='{}', metrics={})\".format(loss, metrics)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_plots_and_summary(fname):\n",
    "    return \\\n",
    "\"\"\"\n",
    "def summarize_diagnostics(history, save_plots):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Classification Accuracy')\n",
    "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "\n",
    "\t# save plot to file\n",
    "\tif save_plots:\n",
    "\t\tfilename = '{}'\n",
    "\t\tpyplot.savefig(filename + '_plot.png')\n",
    "\t\tpyplot.close()\n",
    "\"\"\".format(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(inputs):\n",
    "    \"\"\"\n",
    "    Adds code required to train and evaluate model\n",
    "    Works only if:\n",
    "        - train_data's generator is train_generator\n",
    "        - test_data's generator is called test_generator\n",
    "        - Only epochs and verbose taken from json\n",
    "    \"\"\"\n",
    "    try:\n",
    "            epochs = inputs.get('epochs', 20) #decide default\n",
    "            verbose = inputs.get('verbose', 0)\n",
    "            fit_generator = \\\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\\\n\\\\n ==========Fitting Model========== \\\\n\")\n",
    "history = model.fit_generator(\n",
    "                    train_generator, \n",
    "                    steps_per_epoch=len(train_generator),\n",
    "                    validation_data=test_generator, \n",
    "                    validation_steps=len(test_generator), \n",
    "                    epochs={}, \n",
    "                    verbose={}\n",
    "                )\n",
    "\"\"\".format(epochs, verbose)\n",
    "            \n",
    "            eval_generator = \\\n",
    "\"\"\"            \n",
    "\n",
    "print(\"\\\\n\\\\n ==========Evalutating Model========== \\\\n\")\n",
    "_, acc = model.evaluate_generator(test_generator, steps=len(test_generator), verbose={})\n",
    "print('\\\\n\\\\nACCURACY:  %.3f \\\\n\\\\n' % (acc * 100.0))\n",
    "\"\"\".format(verbose)\n",
    "            return fit_generator + eval_generator\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_summarize_model(inputs):\n",
    "    if eval(inputs.get('plot', False)):\n",
    "        return \\\n",
    "\"\"\"\n",
    "\n",
    "# Plotting graphs and Summarizing Model\n",
    "summarize_diagnostics(history, {})\n",
    "\"\"\".format(eval(inputs.get('save_plots', False)))\n",
    "\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(file_name, input_json):\n",
    "    try:\n",
    "        inputs = json.loads(input_json)\n",
    "        with open(TEST_DIR + file_name, 'w') as f:\n",
    "            f.write(get_imports())\n",
    "            f.write(add_plots_and_summary(\"test\"))\n",
    "            f.write(image_input(inputs))\n",
    "            f.write(init_sequential())\n",
    "            f.write(parse(inputs))\n",
    "            f.write(compile_model(inputs))\n",
    "            f.write(train_evaluate_model(inputs))\n",
    "            f.write(plot_and_summarize_model(inputs))\n",
    "    except:\n",
    "        print(\"Exception occured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_to_file('test.py', my_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
