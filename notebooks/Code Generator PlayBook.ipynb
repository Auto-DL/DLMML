{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import img_to_array,load_img\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.json_to_dict import MakeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Nested JSON\n",
    "with open('SampleJSONs' + os.sep + 'f2b_new_cats_vs_dogs_example.json') as f:\n",
    "    inputs = json.load(f)\n",
    "inputs = MakeDict(inputs).parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers': [{'name': 'Conv2D',\n",
       "   'filters': 32,\n",
       "   'kernel_size': [3, 3],\n",
       "   'activation': 'relu',\n",
       "   'padding': 'same',\n",
       "   'input_shape': [200, 200, 3]},\n",
       "  {'name': 'MaxPooling2D', 'pool_size': [2, 2]},\n",
       "  {'name': 'Flatten'},\n",
       "  {'name': 'Dense',\n",
       "   'units': 128,\n",
       "   'activation': 'relu',\n",
       "   'kernel_initializer': 'he_uniform'},\n",
       "  {'name': 'Dense', 'units': 1, 'activation': 'sigmoid'}],\n",
       " 'dataset': {'type': 'image', 'path': '../data/dogs_and_cats'},\n",
       " 'image': {'augment': {'rotation_range': 40,\n",
       "   'width_shift_range': 0.2,\n",
       "   'height_shift_range': 0.2,\n",
       "   'horizontal_flip': 'True',\n",
       "   'rescale': 0.0039215},\n",
       "  'params': {'target_size': [200, 200],\n",
       "   'batch_size': 64,\n",
       "   'class_mode': 'binary'}},\n",
       " 'optimizer': 'sgd',\n",
       " 'loss': 'binary_crossentropy',\n",
       " 'metrics': ['accuracy'],\n",
       " 'epochs': 5,\n",
       " 'verbose': 1,\n",
       " 'plot': 'True',\n",
       " 'save_plots': 'True'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for all constants\n",
    "\n",
    "TEST_DIR = \"..\" + os.sep + 'test' + os.sep\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"..\" + os.sep + 'data' + os.sep)\n",
    "CATS_DIR = os.path.join(DATA_DIR, 'dogs_and_cats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_table = {\n",
    "    'Conv2D' : 'model.add(Conv2D())',\n",
    "    'MaxPooling2D' : 'model.add(MaxPooling2D())',\n",
    "    'Dense': 'model.add(Dense())',\n",
    "    'Flatten': 'model.add(Flatten())',\n",
    "    'sgd': 'opt = SGD(lr=0.001, momentum=0.9)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imports():\n",
    "    \"\"\"\n",
    "    imports the needed modules \n",
    "    \"\"\"\n",
    "    try:\n",
    "        return \\\n",
    "        \"\"\"\n",
    "# File generated by DLMML Parser\n",
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array,load_img\n",
    "\"\"\"\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_sequential():\n",
    "    \"\"\"\n",
    "    initializes the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return \\\n",
    "'\\nmodel = Sequential()\\n'\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_input(inputs):\n",
    "    \"\"\"\n",
    "    req in input json -\n",
    "    dataset-path -> path to dataset\n",
    "    image-augment -> params for image augmentation if required\n",
    "    params -> params for building generator\n",
    "    \"\"\"\n",
    "    base = inputs['dataset']['path']\n",
    "    test_dir = os.path.join(base, 'test')\n",
    "    train_dir = os.path.join(base, 'train')\n",
    "    \n",
    "    paths = \\\n",
    "\"\"\"\n",
    "\n",
    "base = '{}'\n",
    "train_dir = os.path.join(base, 'train')\n",
    "test_dir = os.path.join(base, 'test')\n",
    "\"\"\".format(inputs['dataset']['path'])\n",
    "\n",
    "    generators = \\\n",
    "\"\"\"\n",
    "augment = {}\n",
    "kwargs = {}\n",
    "\n",
    "train_datagen = ImageDataGenerator(**augment)\n",
    "test_datagen = ImageDataGenerator(**augment)\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, **kwargs)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, **kwargs)\n",
    "\n",
    "\"\"\".format(inputs['image']['augment'], inputs['image']['params'])\n",
    "\n",
    "    return paths+generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(input_dict):\n",
    "    \"\"\"\n",
    "    Parser which adds layers (mostly)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # init_sequential()\n",
    "        layers = input_dict['layers']\n",
    "        generated_code = ''\n",
    "\n",
    "        #TODO: Take care of indentation and stuff if making functions\n",
    "        #TODO: Ordering in dict (work around -> ordered_dict)\n",
    "\n",
    "        for layer in layers:\n",
    "            name = layer.get('name', None)\n",
    "            curr_layer = symbol_table[name]\n",
    "            args = layer.copy()\n",
    "            args.pop('name')\n",
    "            args = str(args)\n",
    "            curr_layer = curr_layer[:-2] + '**' + args + curr_layer[-2:]\n",
    "            generated_code += curr_layer + '\\n'\n",
    "        print()\n",
    "        return generated_code\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(inputs):\n",
    "    \"\"\"\n",
    "    Compiles model along with adding optimizer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        opt = inputs.get(\"optimizer\", \"sgd\")\n",
    "        opt_code = symbol_table[opt]\n",
    "        # let loss and metrics be a compulsory fields for the user?\n",
    "        loss = inputs.get(\"loss\")\n",
    "        metrics = str(inputs.get(\"metrics\"))\n",
    "\n",
    "        return '\\n' + opt_code + '\\n' + \\\n",
    "\"model.compile(optimizer=opt, loss='{}', metrics={})\".format(loss, metrics)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_plots_and_summary(fname=\"test\"):\n",
    "    return \\\n",
    "\"\"\"\n",
    "def summarize_diagnostics(history, save_plots):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "\n",
    "    # save plot to file\n",
    "    if save_plots:\n",
    "        filename = '{}'\n",
    "        pyplot.savefig(filename + '_plot.png')\n",
    "        pyplot.close()\n",
    "\"\"\".format(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(inputs):\n",
    "    \"\"\"\n",
    "    Adds code required to train and evaluate model\n",
    "    Works only if:\n",
    "        - train_data's generator is train_generator\n",
    "        - test_data's generator is called test_generator\n",
    "        - Only epochs and verbose taken from json\n",
    "    \"\"\"\n",
    "    try:\n",
    "            epochs = inputs.get('epochs', 20) #decide default\n",
    "            verbose = inputs.get('verbose', 0)\n",
    "            fit_generator = \\\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\\\n\\\\n ==========Fitting Model========== \\\\n\")\n",
    "history = model.fit_generator(\n",
    "                    train_generator, \n",
    "                    steps_per_epoch=len(train_generator),\n",
    "                    validation_data=test_generator, \n",
    "                    validation_steps=len(test_generator), \n",
    "                    epochs={}, \n",
    "                    verbose={}\n",
    "                )\n",
    "\"\"\".format(epochs, verbose)\n",
    "            \n",
    "            eval_generator = \\\n",
    "\"\"\"            \n",
    "\n",
    "print(\"\\\\n\\\\n ==========Evalutating Model========== \\\\n\")\n",
    "_, acc = model.evaluate_generator(test_generator, steps=len(test_generator), verbose={})\n",
    "print('\\\\n\\\\nACCURACY:  %.3f \\\\n\\\\n' % (acc * 100.0))\n",
    "\"\"\".format(verbose)\n",
    "            return fit_generator + eval_generator\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_summarize_model(inputs):\n",
    "    if eval(inputs.get('plot', False)):\n",
    "        return \\\n",
    "\"\"\"\n",
    "\n",
    "# Plotting graphs and Summarizing Model\n",
    "summarize_diagnostics(history, {})\n",
    "\"\"\".format(eval(inputs.get('save_plots', False)))\n",
    "\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(path_model=\"model.hdf5\"):\n",
    "    return \\\n",
    "\"\"\"\n",
    "\n",
    "# Saving Model\n",
    "model.save('{}')\n",
    "print(\"\\\\n Model saved at {}\")\n",
    "\"\"\".format(path_model, path_model)\n",
    "\n",
    "def generate_image_to_array_converter(inputs):\n",
    "    test_dir = inputs.get('test_dir', '../data/dogs_and_cats/test/dogs/dog.2011.jpg')\n",
    "\n",
    "    return \\\n",
    "'''\n",
    "\n",
    "def convert_image_to_array(input_dir = '{}'):\n",
    "    img = load_img(input_dir, target_size = {})\n",
    "    array = img_to_array(img)\n",
    "    array = numpy.array([array])\n",
    "\n",
    "    return array\n",
    "'''.format(test_dir, inputs['image']['params']['target_size'])\n",
    "\n",
    "def load_model(path_model=\"model.hdf5\"):\n",
    "    return \\\n",
    "'''\n",
    "\n",
    "print(\"\\\\n\\\\n ==========Loading Model========== \\\\n\")\n",
    "model = load_model('{}')\n",
    "'''.format(path_model)\n",
    "\n",
    "def predict():\n",
    "    \n",
    "    return \\\n",
    "'''\n",
    "\n",
    "to_pred = convert_image_to_array()\n",
    "print(\"\\\\n\\\\nPredicted class is  \", model.predict_classes(to_pred))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(file_name, input_dict):\n",
    "    \"\"\"\n",
    "    file_name: string (with extension), generated code \\\n",
    "               will be saved as file_name in the test dir\n",
    "    input_dict: Dict generated by parsing json to make \\\n",
    "                nested dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inputs = input_dict\n",
    "        with open(TEST_DIR + file_name, 'w') as f:\n",
    "            f.write(get_imports())\n",
    "            f.write(add_plots_and_summary())\n",
    "            f.write(image_input(inputs))\n",
    "            f.write(init_sequential())\n",
    "            f.write(parse(inputs))\n",
    "            f.write(compile_model(inputs))\n",
    "            f.write(train_evaluate_model(inputs))\n",
    "            f.write(plot_and_summarize_model(inputs))\n",
    "            f.write(save_model())\n",
    "            f.write(generate_image_to_array_converter(inputs))\n",
    "            f.write(load_model())\n",
    "            f.write(predict())\n",
    "\n",
    "        print(\"Code generated in file test/\" + file_name)\n",
    "    except Exception as e:\n",
    "        print(\"Exception occured\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Code generated in file test/test.py\n"
     ]
    }
   ],
   "source": [
    "write_to_file('test.py', inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
